#!/usr/bin/env python
'''
    Python script that runs the job on the worker node and is started
    within the DailyPythonScripts directory
'''
import pickle
from optparse import OptionParser
from jobtypes import *
import shutil
import os
from dps.utils.file_utilities import make_folder_if_not_exists

# from unfolding_pull_job import UnfoldingPullJob
parser = OptionParser(__doc__)
parser.add_option("-v", "--verbose", dest="verbose", action="store_true",
                  help="Show lots of logging")
(options, args) = parser.parse_args()
pickle_file_name = args[0]
job_id = args[1]
subjob_id = args[2]
n_jobs = args[3]
print 'Job file:', pickle_file_name
print 'sub job ID:', subjob_id
print 'job ID:', job_id
print 'n_jobs:', n_jobs
subjob_id = int(subjob_id)
output_subjob_id = subjob_id
n_jobs = int(n_jobs)

pickle_file = open(pickle_file_name)
job = pickle.load(pickle_file)
# copy additional input files to where they should be
additional_input_files = job.additional_input_files
base_dir = os.environ['_CONDOR_JOB_IWD']
for f in additional_input_files:
    file_name = f.split('/')[-1]
    folder = f.replace(file_name, '')
    make_folder_if_not_exists(folder)
    shutil.copyfile(f, base_dir + '/' + file_name)

subjobs = job.split(n_jobs)
# pick the correct subjob
j = subjobs[subjob_id]
if job.filter_jobs:
    print 'Job filter:', job.filter_jobs
    subjob_id = job.filter_jobs[subjob_id]
    j = subjobs[subjob_id]
    print 'new sub job ID:', subjob_id

j.run()

output_tar_file = j.tar_output(job_id, subjob_id)
output_file_name = output_tar_file.split('/')[-1]
shutil.copyfile(output_tar_file, base_dir + '/' + output_file_name)
